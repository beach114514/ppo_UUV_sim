import os
import time
import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm
import gymnasium
from gymnasium.vector import AsyncVectorEnv

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
# è¯·ç¡®ä¿è¿™ä¸¤ä¸ªæ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹
from PPO_Algorithm import PPOContinuous, PPOBuffer, get_log_prob_batch
from UUV_Env import UUV_MultiGoal_Env, custom_multi_goal_reward
import rl_utils  # å‡è®¾ä½ æœ‰ä¸€ä¸ªç®€å•çš„ moving_average å‡½æ•°ï¼Œå¦‚æœæ²¡æœ‰ï¼Œæˆ‘åœ¨ä¸‹é¢æä¾›äº†ä¸€ä¸ªç®€å•çš„å®ç°


# ==========================================
# 0. ç®€å•çš„ç§»åŠ¨å¹³å‡å·¥å…· (å¦‚æœä½ æ²¡æœ‰ rl_utils)
# ==========================================
def moving_average(a, window_size):
    cumulative_sum = np.cumsum(np.insert(a, 0, 0))
    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size
    r = np.arange(1, window_size - 1, 2)
    begin = np.cumsum(a[:window_size - 1])[::2] / r
    end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]
    return np.concatenate((begin, middle, end))


# ==========================================
# 1. è¶…å‚æ•°é…ç½®
# ==========================================
# --- ç¯å¢ƒè®¾ç½® ---
NUM_ENVS = 16  # å¹¶è¡Œç¯å¢ƒæ•°é‡ (CPUæ ¸å¿ƒæ•°è¶Šå¤šè¶Šå¥½)
MAX_STEPS = 1000  # å•å›åˆæœ€å¤§æ­¥æ•° (ç»™è¶³å¤Ÿçš„æ—¶é—´åˆ°è¾¾æ‰€æœ‰ç›®æ ‡)
GOAL_POSITIONS = [(300, 100), (-100, 200), (0, -400)]
OBSTACLES = []  # (x, y, r)

# --- è®­ç»ƒè®¾ç½® ---
TOTAL_EPISODES = 8000  # æ€»è®­ç»ƒå›åˆæ•° (ç”¨äºæ§åˆ¶æ€»æ—¶é•¿)
STEPS_PER_UPDATE = 2048  # æ¯æ¬¡ PPO æ›´æ–°æ”¶é›†çš„æ­¥æ•° (å¿…é¡»æ˜¯ NUM_ENVS çš„å€æ•°)
REWARD_SCALE = 100.0  # å¥–åŠ±ç¼©æ”¾åˆ†æ¯ (ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼ŒPPOå†…éƒ¨å¯èƒ½æœ‰è‡ªå·±çš„ç¼©æ”¾)

# --- PPO è¶…å‚æ•° ---
ACTOR_LR = 1e-5
CRITIC_LR = 5e-6
HIDDEN_DIM = 256
GAMMA = 0.99
LAMBDA = 0.95
EPOCHS = 10
EPS_CLIP = 0.2
ENT_COEF = 0.01  # ç†µç³»æ•°

# --- ä¿å­˜ä¸è¯„ä¼° ---
SAVE_INTERVAL = 50  # æ¯å¤šå°‘æ¬¡ Update ä¿å­˜ä¸€æ¬¡æ¨¡å‹
EVAL_INTERVAL = 20  # æ¯å¤šå°‘æ¬¡ Update è¿›è¡Œä¸€æ¬¡å¯è§†åŒ–è¯„ä¼°
MODEL_DIR = './model_save'


# ==========================================
# 2. è¾…åŠ©å‡½æ•°
# ==========================================

def make_env(rank, render_mode='none'):
    """
    åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°ï¼Œç”¨äº AsyncVectorEnv
    rank: ç¯å¢ƒçš„ç´¢å¼•ï¼Œç”¨äºè®¾ç½®ä¸åŒçš„éšæœºç§å­
    """

    def _init():
        env = UUV_MultiGoal_Env(
            GOAL_POSITIONS,
            custom_multi_goal_reward,
            OBSTACLES,
            render_mode=render_mode,
            max_steps=MAX_STEPS
        )
        # ä¸ºæ¯ä¸ªç¯å¢ƒè®¾ç½®ä¸åŒçš„ç§å­ï¼Œé˜²æ­¢å¹¶è¡Œç¯å¢ƒè¿è¡Œä¸€æ¨¡ä¸€æ ·
        env.reset(seed=rank + 1000)
        return env

    return _init


def evaluate_policy(agent, device, render=False):
    """
    è¯„ä¼°å‡½æ•°ï¼šæš‚åœå¹¶è¡Œè®­ç»ƒï¼Œä½¿ç”¨å•ç¯å¢ƒæµ‹è¯•å½“å‰ç­–ç•¥
    render: æ˜¯å¦å¼€å¯å¯è§†åŒ–çª—å£
    """
    # åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„è¯„ä¼°ç¯å¢ƒ
    env = UUV_MultiGoal_Env(GOAL_POSITIONS, custom_multi_goal_reward, OBSTACLES,
                            render_mode='human' if render else 'none', max_steps=MAX_STEPS)

    state, _ = env.reset(seed=42)  # å›ºå®šç§å­ä»¥ä¾¿å¯¹æ¯”
    done = False
    total_reward = 0
    steps = 0

    while not done:
        if render:
            env.render()

        # å‡†å¤‡çŠ¶æ€æ•°æ®ï¼š(State_Dim,) -> (1, State_Dim)
        state_tensor = np.array([state])

        # è·å–åŠ¨ä½œ (ä½¿ç”¨ take_action)
        # æ³¨æ„ï¼štake_action è¿”å› batch å½¢å¼ï¼Œæˆ‘ä»¬éœ€è¦å– [0]
        # è¯„ä¼°æ—¶æˆ‘ä»¬ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä¹Ÿä¸éœ€è¦æ¢ç´¢å™ªå£°ï¼Œä½†è¿™é‡Œå¤ç”¨ take_action é‡‡æ ·ä¹Ÿå¯ä»¥
        # å¦‚æœæƒ³è¦ç¡®å®šæ€§ç­–ç•¥ï¼Œå¯ä»¥ä¿®æ”¹ Agent å¢åŠ  deterministic æ¨¡å¼ï¼Œæˆ–è€…ç›´æ¥ç”¨ mean
        with torch.no_grad():
            _, action_phys = agent.take_action(state_tensor)

        # ç¯å¢ƒæ‰§è¡Œç‰©ç†åŠ¨ä½œ
        action = action_phys[0]
        next_state, reward, term, trunc, _ = env.step(action)

        done = term or trunc
        total_reward += reward
        state = next_state
        steps += 1

    env.close()
    return total_reward, steps


# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
# ================================================================
# ç”¨äºç›´æ¥è¿è¡Œæœ¬æ–‡ä»¶è¿›è¡Œè®­ç»ƒçš„ä¸»ç¨‹åº (é€»è¾‘ä¸ main.py ä¸€è‡´)
# ================================================================
if __name__ == '__main__':
    import os
    import gymnasium
    import numpy as np
    import torch
    import matplotlib.pyplot as plt
    from tqdm import tqdm
    from gymnasium.vector import AsyncVectorEnv

    # å¼•å…¥ç¯å¢ƒ (PPO_Algorithm.py å¤´éƒ¨å·²ç»å¼•å…¥äº†éƒ¨åˆ†å®å®šä¹‰ï¼Œè¿™é‡Œè¡¥å……å¼•å…¥ Env ç±»)
    # ç¡®ä¿ UUV_Env.py åœ¨åŒä¸€ç›®å½•ä¸‹
    try:
        from UUV_Env import UUV_MultiGoal_Env, custom_multi_goal_reward
    except ImportError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° UUV_Env.pyï¼Œæ— æ³•è¿è¡Œå®Œæ•´è®­ç»ƒå¾ªç¯ã€‚")
        exit()


    # --- 0. è¾…åŠ©å‡½æ•°: ç§»åŠ¨å¹³å‡ ---
    def moving_average(a, window_size):
        cumulative_sum = np.cumsum(np.insert(a, 0, 0))
        middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size
        r = np.arange(1, window_size - 1, 2)
        begin = np.cumsum(a[:window_size - 1])[::2] / r
        end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]
        return np.concatenate((begin, middle, end))


    # --- 1. è¶…å‚æ•°é…ç½® ---
    # ç¯å¢ƒé…ç½®
    NUM_ENVS = 16  # å¹¶è¡Œç¯å¢ƒæ•°é‡
    MAX_STEPS = 700  # å•å›åˆæœ€å¤§æ­¥æ•°
    GOAL_POSITIONS = [(300, 100), (-100, 200), (0, -400)]
    OBSTACLES = []

    # è®­ç»ƒé…ç½®
    TOTAL_EPISODES = 5000  # æ€»è®­ç»ƒå›åˆæ•°
    STEPS_PER_UPDATE = 2048  # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
    REWARD_SCALE = 100.0  # å¥–åŠ±ç¼©æ”¾
    REWARD_SHIFT = 0.0

    # PPO å‚æ•°
    ACTOR_LR = 1e-5
    CRITIC_LR = 5e-6
    HIDDEN_DIM = 256
    GAMMA = 0.99
    LAMBDA = 0.95
    EPOCHS = 10
    EPS_CLIP = 0.2
    ENT_COEF = 0.01

    # ä¿å­˜è·¯å¾„
    MODEL_DIR = './model_save_algo_run'
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)


    # --- 2. ç¯å¢ƒå·¥å‚å‡½æ•° ---
    def make_env(rank):
        def _init():
            env = UUV_MultiGoal_Env(
                GOAL_POSITIONS,
                custom_multi_goal_reward,
                OBSTACLES,
                render_mode='none',
                max_steps=MAX_STEPS
            )
            env.reset(seed=rank + 2000)
            return env

        return _init


    # --- 3. åˆå§‹åŒ– ---
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    print(f"ğŸš€ [PPO_Algorithm Direct Run] Start on {device} with {NUM_ENVS} environments.")

    # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
    uuv_env = AsyncVectorEnv([make_env(i) for i in range(NUM_ENVS)])

    state_dim = uuv_env.single_observation_space.shape[0]
    action_dim = uuv_env.single_action_space.shape[0]

    # åˆå§‹åŒ– Agent
    agent = PPOContinuous(state_dim, HIDDEN_DIM, action_dim,
                          ACTOR_LR, CRITIC_LR, LAMBDA, EPOCHS, EPS_CLIP, GAMMA,
                          device, ent_coef=ENT_COEF)

    buffer = PPOBuffer()

    # å˜é‡è®°å½•
    return_list = []
    episode_returns = np.zeros(NUM_ENVS)
    finished_episodes = 0
    total_steps = 0

    # åˆå§‹çŠ¶æ€
    states, _ = uuv_env.reset()
    pbar = tqdm(total=TOTAL_EPISODES, desc="Training")

    try:
        while finished_episodes < TOTAL_EPISODES:

            # === é˜¶æ®µ 1: æ•°æ®æ”¶é›† ===
            for _ in range(STEPS_PER_UPDATE // NUM_ENVS):

                # è·å–åŠ¨ä½œ (Norm ç”¨äºè®­ç»ƒ, Phys ç”¨äºæ‰§è¡Œ)
                action_norm, action_phys = agent.take_action(states)

                # è®¡ç®—æ—§ç­–ç•¥ LogProb
                log_probs = get_log_prob_batch(agent, states, action_norm)

                # ç¯å¢ƒæ­¥è¿›
                next_states, rewards, terminations, truncations, infos = uuv_env.step(action_phys)
                dones = terminations | truncations

                # å­˜å…¥ Buffer
                buffer.push(states, action_norm, rewards, next_states, dones, log_probs)

                # è®°å½•å¥–åŠ±
                episode_returns += rewards

                # å¤„ç†ç»“æŸçš„å›åˆ
                for i, done in enumerate(dones):
                    if done:
                        return_list.append(episode_returns[i] / REWARD_SCALE)
                        episode_returns[i] = 0
                        finished_episodes += 1
                        pbar.update(1)

                states = next_states
                total_steps += NUM_ENVS

            # === é˜¶æ®µ 2: æ›´æ–° ===
            transition_dict = buffer.get_data()
            agent.update(transition_dict)
            buffer.clear()

            # æ˜¾ç¤ºè¿›åº¦
            if len(return_list) > 0:
                pbar.set_postfix({'AvgRet': f'{np.mean(return_list[-10:]):.2f}'})

    except KeyboardInterrupt:
        print("\nğŸ›‘ Training interrupted.")
    finally:
        pbar.close()
        uuv_env.close()

    # --- 4. ç»˜å›¾ ---
    print("Drawing training curve...")
    plt.figure(figsize=(12, 6))
    plt.plot(return_list, alpha=0.3, color='gray', label='Raw')
    if len(return_list) > 10:
        mv_return = moving_average(return_list, 19)
        plt.plot(mv_return, color='red', linewidth=2, label='Moving Avg')
    plt.title("PPO Training Curve (Algorithm File Run)")
    plt.xlabel("Episodes")
    plt.ylabel("Reward")
    plt.legend()
    plt.savefig(os.path.join(MODEL_DIR, 'algo_run_curve.png'))
    plt.show()
    print("Done.")